{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T09:23:34.526738Z",
     "start_time": "2022-05-16T09:23:32.158127Z"
    }
   },
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T09:58:28.843613Z",
     "start_time": "2022-05-22T09:58:21.674211Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from cvxopt import matrix as cvxopt_matrix\n",
    "from cvxopt import solvers as cvxopt_solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC(classification) Class \n",
    "\n",
    "hyperplane을 얻는데 필요한 Convex 최적화 알고리즘을 제공하는 cvxopt를 사용하였다.  \n",
    "SVC(support vector classification)클래스를 생성하였다. kernel은 rbf, polynomial, sigmoid를 사용하였으며 세가지 모두 사용하지 않는다면 Linear & Soft margin을 사용하도록 설정했다. \n",
    "\n",
    "전처리를 위한 함수로 make_label_map 함수, 원 데이터의 라벨을 실제로 1, -1로 변환하는 transform_label 함수, 1과 -1을 원 데이터의 라벨로 바꿔주는 inverse_label 함수, 두 벡터의 kernel 값을 계산하는 get_kernel_val 함수를 생성하였다. \n",
    "\n",
    "위 함수에서 X, y 가 정의되면 P, q, G, h, A, b를 numpy array 형식으로 작성하였으며 형식은 다음과 같다. \n",
    "\n",
    "$$\n",
    "\\begin{align} x &=\\alpha\\;\\; (n \\times 1 \\; \\text{vector}), \\\\ P&=K \\;\\;(n\\times n \\; \\text{matrix}), \\\\ q &= -e \\;\\; (n \\times 1 \\; \\text{vector}), \\\\ G &=\\begin{pmatrix} -I \\\\ I \\end{pmatrix} \\;\\;  (n \\times n \\; \\text{matrix}), \\\\ h &= (0, \\ldots, 0, C, \\ldots, C)^t \\;\\; (2n \\times 1 \\;\\text{vector}), \\\\ A &= y^t \\;\\;(1\\times n \\;\\text{matrix}), \\\\ b &= 0 \\;\\;(\\text{scalar}) \\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVC():\n",
    "    def __init__(self, kernel=None, gamma = None, coef0=0):\n",
    "        self.labels_map = None\n",
    "        self.kernel = kernel\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.alphas = None\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.gamma = gamma\n",
    "        self.coef0 = coef0\n",
    "        if kernel is not None:\n",
    "            assert kernel in [ 'rbf','polynomial', 'sigmoid']\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        uniq_labels = np.unique(y)\n",
    "        assert len(uniq_labels) == 2\n",
    "        \n",
    "            \n",
    "        self.make_label_map(uniq_labels)\n",
    "        self.X = X\n",
    "        \n",
    "        if not self.gamma:\n",
    "            gamma = 1/(X.shape[1]*X.var())\n",
    "        \n",
    "        y = [self.transform_label(label, self.labels_map) for label in y] ## 1, -1로 변환\n",
    "        y = np.array(y)\n",
    "        ## formulating standard form\n",
    "        m, n = X.shape\n",
    "        y = y.reshape(-1,1)*1.\n",
    "        self.y = y\n",
    "        if self.kernel is not None:\n",
    "            K = np.zeros((m,m))\n",
    "            for i in range(m):\n",
    "                for j in range(m):\n",
    "                    K[i][j] = y[i]*y[j]*self.get_kernel_val(X[i], X[j])                       \n",
    "        else:\n",
    "            yX = y*X\n",
    "            K = np.dot(yX,yX.T)\n",
    "        P = cvxopt_matrix(K)\n",
    "        q = cvxopt_matrix(-np.ones((m, 1)))\n",
    "        G = cvxopt_matrix(np.vstack((np.eye(m)*-1,np.eye(m))))\n",
    "        h = cvxopt_matrix(np.hstack((np.zeros(m), np.ones(m) * C)))\n",
    "        A = cvxopt_matrix(y.reshape(1, -1))\n",
    "        b = cvxopt_matrix(np.zeros(1))\n",
    "        ## cvxopt configuration\n",
    "        cvxopt_solvers.options['show_progress'] = False ## 결과 출력 X\n",
    "        sol = cvxopt_solvers.qp(P, q, G, h, A, b) \n",
    "        alphas = np.array(sol['x'])\n",
    "        S = (alphas>1e-4).flatten()\n",
    "        \n",
    "        \n",
    "        if self.kernel is not None:\n",
    "            sum_val = 0\n",
    "            S_index = np.where(S==True)[0]\n",
    "            for s in S_index:\n",
    "                temp_vec = np.array([self.get_kernel_val(z, X[s]) for z in X])\n",
    "                temp_vec = np.expand_dims(temp_vec, axis=1)\n",
    "                sum_val += np.sum(y[s] - np.sum(y*alphas*temp_vec))\n",
    "            b = sum_val/len(S)\n",
    "            self.b = b\n",
    "        else:\n",
    "            w = ((y*alphas).T@X).reshape(-1,1)\n",
    "            b = np.mean(y[S] - np.dot(X[S],w))\n",
    "            self.w = w\n",
    "            self.b = b\n",
    "        self.alphas = alphas\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict(x) for x in X]\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def _predict(self, x):\n",
    "        if self.kernel:\n",
    "            temp_vec = np.array([self.get_kernel_val(x, y) for y in self.X])\n",
    "            temp_vec = np.expand_dims(temp_vec, axis=1)\n",
    "            S = (self.alphas>1e-4).flatten()\n",
    "            res = np.sign(np.sum(self.y[S]*self.alphas[S]*temp_vec[S])+self.b)\n",
    "        else:\n",
    "            res = np.sign(self.w.T.dot(x)+self.b)\n",
    "        res = self.inverse_label(res, self.labels_map)\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def make_label_map(self, uniq_labels):\n",
    "        labels_map = list(zip([-1, 1], uniq_labels))\n",
    "        self.labels_map = labels_map\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def transform_label(self, label, labels_map):\n",
    "        res = [l[0] for l in labels_map if l[1] == label][0]\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def inverse_label(self, svm_label, labels_map):\n",
    "        try:\n",
    "            res = [l[1] for l in labels_map if l[0] == svm_label][0]\n",
    "        except:\n",
    "            print(svm_label)\n",
    "            print(labels_map)\n",
    "            raise\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def get_kernel_val(self, x, y):\n",
    "        X = self.X\n",
    "        coef0 = self.coef0\n",
    "        if not self.gamma:\n",
    "            gamma = 1/(X.shape[1]*X.var())\n",
    "            \n",
    "        if self.kernel == 'rbf':\n",
    "            return np.exp(-gamma*np.square(np.linalg.norm(x-y)))\n",
    "        elif self.kernel == 'polynomial':\n",
    "            return (gamma*np.dot(x,y)+coef0)**2\n",
    "        else:\n",
    "            return np.tanh(gamma*np.dot(x,y)+coef0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM class\n",
    "\n",
    "SVM클래스는 분류와 회귀를 동시에 수행할 수 있도록 구현하였다. type을 통해서 classification 혹은 regression을 실행하도록 하였다. 마찬가지로 두 벡터에 대한 kernel값을 계산하는 함수가 필요하다. \n",
    "SVC는 classification을 위한 함수, SVR은 regression을 위한 함수로 구성하였다. \n",
    "SVC에서는 앞선 SVC 클래스에서 모형을 적합하여 SVM의 model_list에 저장한다. \n",
    "\n",
    "cvxopt에서 계산하는 식은 다음과 같다. \n",
    "\n",
    "$$\n",
    "\\begin{align} x &=(\\alpha^t, \\alpha^{*t})^t \\;\\; (2n \\times 1 \\; \\text{vector}), \\\\ P&=(I -I)^tQ(I -I) \\;\\;(2n\\times 2n \\; \\text{matrix}), \\\\ q &= \\epsilon e_{2n} - (y^t, -y^t)^t \\;\\; (2n \\times 1 \\; \\text{vector}), \\\\ G &=\\begin{pmatrix} -I & O \\\\ I & O \\\\ O & -I \\\\ O & I \\end{pmatrix} \\;\\;  (4n \\times 2n \\; \\text{matrix}), \\\\ h &= (0, \\ldots, 0, C, \\ldots, C, 0, \\ldots, 0, C, \\ldots, C)^t \\;\\; (4n \\times 1 \\;\\text{vector}), \\\\ A &= e_n^t(I -I) \\;\\;(1\\times 2n \\;\\text{matrix}), \\\\ b &= 0 \\;\\;(\\text{scalar}) \\end{align}\n",
    "$$\n",
    "\n",
    "predict, predict_reg 함수를 통해 예측을 하고 Accuracy를 통해 학습 정확도를 계산하였다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM():\n",
    "    def __init__(self, type='classification', kernel=None, gamma=None, C=None, coef0=0):\n",
    "        assert type in ['classification', 'regression']\n",
    "        self.type = type\n",
    "        self.kernel=kernel\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.coef0 = coef0\n",
    "        self.model_list = None\n",
    "        self.gamma = gamma\n",
    "        self.C = C\n",
    "        if kernel is not None:\n",
    "            assert kernel in ['polynomial', 'rbf', 'sigmoid']\n",
    "            \n",
    "    def fit(self, X, y, epsilon=[0.1, 0.01, 0.001]):\n",
    "        if self.type == 'classification':\n",
    "            self._fit_svc(X, y)\n",
    "        else:\n",
    "            self._fit_svr(X, y, epsilon)\n",
    "            \n",
    "    def _fit_svc(self, X, y):\n",
    "        uniq_labels = np.unique(y)\n",
    "        label_combinations = list(combinations(uniq_labels, 2))\n",
    "        model_list = []\n",
    "        for lc in label_combinations:\n",
    "            target_idx = np.array([x in lc for x in y])\n",
    "            y_restricted = y[target_idx]\n",
    "            X_restricted = X[target_idx]\n",
    "            clf = SVC(kernel=self.kernel, coef0=self.coef0)\n",
    "            clf.fit(X_restricted, y_restricted)\n",
    "            model_list.append(clf)\n",
    "        \n",
    "        self.model_list = model_list\n",
    "        return\n",
    "    \n",
    "    def _fit_svr(self, X, y, epsilon):\n",
    "\n",
    "        assert epsilon > 0\n",
    "        self.X = X\n",
    "        m, n = X.shape\n",
    "        y = y.reshape(-1,1)*1.\n",
    "        self.y = y\n",
    "        if self.kernel is not None:\n",
    "            K = np.zeros((m,m))\n",
    "            for i in range(m):\n",
    "                for j in range(m):\n",
    "                    K[i][j] = self.get_kernel_val(X[i], X[j])\n",
    "        else:\n",
    "            K = X.dot(X.T)\n",
    "        I = np.eye(m)\n",
    "        O = np.zeros((m, m))\n",
    "        sub_K = np.hstack([I, -I])\n",
    "        main_K = sub_K.T.dot(K.dot(sub_K))\n",
    "        P = cvxopt_matrix(main_K)\n",
    "        q = cvxopt_matrix(epsilon*np.ones((2*m, 1)) - np.vstack([y, -y]))\n",
    "\n",
    "        G = np.vstack([np.hstack([-I, O]), np.hstack([I, O]), np.hstack([O, -I]), np.hstack([O, I])])\n",
    "        G = cvxopt_matrix(G)\n",
    "        h = cvxopt_matrix(np.hstack([np.zeros(m), C*np.ones(m)]*2))\n",
    "        A = cvxopt_matrix(np.ones((m,1)).T.dot(sub_K))\n",
    "        b = cvxopt_matrix(np.zeros(1))\n",
    "        \n",
    "        cvxopt_solvers.options['show_progress'] = False\n",
    "        sol = cvxopt_solvers.qp(P, q, G, h, A, b)\n",
    "        sol_root = np.array(sol['x'])\n",
    "        alphas = sol_root[:m]\n",
    "        alphas_star = sol_root[m:]\n",
    "        \n",
    "        S = (alphas>1e-4).flatten()       \n",
    "        if self.kernel is not None:\n",
    "            sum_val = []\n",
    "            S_index = np.where(S==True)[0]\n",
    "            for s in S_index:\n",
    "                temp_vec = np.array([self.get_kernel_val(z, X[s]) for z in X])\n",
    "                temp_vec = np.expand_dims(temp_vec, axis=1)\n",
    "                sum_val.append(-epsilon + np.sum(y[s] - np.sum((alphas-alphas_star)*temp_vec)))\n",
    "            b = min(sum_val)\n",
    "            self.b = b\n",
    "\n",
    "        else:\n",
    "            w = alphas.T.dot(X)-alphas_star.T.dot(X)\n",
    "            w = w.reshape(-1,1)\n",
    "            b = -epsilon+np.min(y[S] - np.dot(X[S],w))\n",
    "            self.w = w\n",
    "            self.b = b\n",
    "        self.alphas = sol_root\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.type =='classification':\n",
    "            model_list = self.model_list\n",
    "            prediction = [model.predict(X) for model in model_list]\n",
    "            prediction = [Counter(pred).most_common(1)[0][0] for pred in list(zip(*prediction))]\n",
    "        else:\n",
    "            prediction = [self._predict_reg(x) for x in X]\n",
    "        return prediction\n",
    "    \n",
    "    def _predict_reg(self, x):\n",
    "        X = self.X\n",
    "        \n",
    "        if self.kernel is not None:\n",
    "            m, _ = self.X.shape\n",
    "            sol_root = self.alphas\n",
    "            alphas = sol_root[:m]\n",
    "            alphas_star = sol_root[m:]\n",
    "            \n",
    "            temp_vec = np.array([self.get_kernel_val(z, x) for z in X])\n",
    "            temp_vec = np.expand_dims(temp_vec, axis=1)\n",
    "            pred = np.sum((alphas-alphas_star)*temp_vec)+self.b\n",
    "        else:\n",
    "            w = self.w\n",
    "            b = self.b\n",
    "            pred = w.dot(x)+b\n",
    "            pred = pred[0]\n",
    "        return pred\n",
    "    \n",
    "    def get_kernel_val(self, x, y):\n",
    "        X = self.X\n",
    "        coef0 = self.coef0\n",
    "        \n",
    "        if not self.gamma:\n",
    "            gamma = 1/(X.shape[1]*X.var())\n",
    "            \n",
    "        if self.kernel == 'rbf':\n",
    "            return np.exp(-gamma*np.square(np.linalg.norm(x-y)))\n",
    "        elif self.kernel == 'polynomial':\n",
    "            return (gamma*np.dot(x,y)+coef0)**2 \n",
    "        elif self.kernel == 'sigmoid':\n",
    "            return np.tanh(gamma*np.dot(x,y)+coef0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "분류를 위한 데이터는 붓꽃 데이터를 이용하였다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['species'] = [iris.target_names[x] for x in iris.target]\n",
    " \n",
    "species_to_labels = dict(zip(df['species'].unique(), range(len(df['species'].unique()))))\n",
    "df['species'] = df['species'].map(species_to_labels)\n",
    "df = df.rename(columns={'species':'label'})\n",
    "\n",
    "X = df.drop('label', axis=1).values\n",
    "y = df['label'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear & Soft margin\n",
    "C값에 따른 차이를 보기 위해 C = [0.1, 1, 10, 100, 1000]으로 Accuracy값을 확인하였다. \n",
    "확인 결과 C값에 따른 정확도의 차이는 거의 없다고 봐도 무방한 것 같다.  \n",
    "\n",
    "마진 관점에서는 C값이 클수록 $/xi$값이 작아지고 마진이 좁아지며, C값이 작을수록 $/xi$가 커지고 마진도 커진다. 데이터때문인가 마진 값이 정확도에 미치는 영향을 알아보기 위해서는 다른 데이터를 사용하거나 이미지를 생성해봐야 알거 같다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel: Linear C: 0.1 Accuracy: 0.98\n",
      "kernel: Linear C: 1 Accuracy: 0.9866666666666667\n",
      "kernel: Linear C: 10 Accuracy: 0.98\n",
      "kernel: Linear C: 100 Accuracy: 0.98\n",
      "kernel: Linear C: 1000 Accuracy: 0.9866666666666667\n"
     ]
    }
   ],
   "source": [
    "for kernel in [None]:\n",
    "    for gamma in [None]:\n",
    "        for C in [0.1, 1, 10, 100, 1000]:\n",
    "            clf_linear = SVM(type='classification', kernel=None, gamma=None, C=None)\n",
    "            clf_linear.fit(X, y)\n",
    "            pred_linear = clf_linear.predict(X)\n",
    "            print('kernel: Linear', 'C:', C, 'Accuracy:', np.mean(y==pred_linear))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear\n",
    "Kernel은 [rbf, polynomial, sigmoid] 3가지를 사용하였으며, gamma값은 [0.001, 0.01, 0.1, 1] 4가지, C값은 linear와 마찬가지로 [0.1, 1, 10, 100, 1000] 5가지를 사용하였다. \n",
    "\n",
    "결과는 다음과 같이 kernel은 rbf일때 C값은 10일때 Accuracy가 가장 높았으며, gamma값은 크게 영향을 받지 않는 것으로 확인했다. 그 이유는 iris data가 보다 단순한 편에 속하기 때문인 것 같다. \n",
    "\n",
    "이외에도 kernel이 polynomial, sigmoid인 경우에는 Accuracy가 높지 않았다. 따라서 성능에 영향을 미치는 우선순위를 고려한다면 Kernel > C > gamma 인 것 같다.\n",
    "\n",
    "\n",
    "| Kernel | C | Gamma | Accuracy |\n",
    "| :------: | :-: | :-----: | :--------: |\n",
    "| rbf | 10 | 0.001 | 0.987 |\n",
    "| rbf | 10 | 0.01 | 0.987 |\n",
    "| rbf | 10 | 0.1 | 0.987 |\n",
    "| rbf | 10 | 1 | 0.987 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel: rbf C: 0.1 gamma: 0.001 Accuracy: 0.9466666666666667\n",
      "kernel: rbf C: 1 gamma: 0.001 Accuracy: 0.9666666666666667\n",
      "kernel: rbf C: 10 gamma: 0.001 Accuracy: 0.9866666666666667\n",
      "kernel: rbf C: 100 gamma: 0.001 Accuracy: 0.9333333333333333\n",
      "kernel: rbf C: 1000 gamma: 0.001 Accuracy: 0.84\n",
      "kernel: rbf C: 0.1 gamma: 0.01 Accuracy: 0.9466666666666667\n",
      "kernel: rbf C: 1 gamma: 0.01 Accuracy: 0.9666666666666667\n",
      "kernel: rbf C: 10 gamma: 0.01 Accuracy: 0.9866666666666667\n",
      "kernel: rbf C: 100 gamma: 0.01 Accuracy: 0.9333333333333333\n",
      "kernel: rbf C: 1000 gamma: 0.01 Accuracy: 0.84\n",
      "kernel: rbf C: 0.1 gamma: 0.1 Accuracy: 0.9466666666666667\n",
      "kernel: rbf C: 1 gamma: 0.1 Accuracy: 0.9666666666666667\n",
      "kernel: rbf C: 10 gamma: 0.1 Accuracy: 0.9866666666666667\n",
      "kernel: rbf C: 100 gamma: 0.1 Accuracy: 0.9333333333333333\n",
      "kernel: rbf C: 1000 gamma: 0.1 Accuracy: 0.84\n",
      "kernel: rbf C: 0.1 gamma: 1 Accuracy: 0.9466666666666667\n",
      "kernel: rbf C: 1 gamma: 1 Accuracy: 0.9666666666666667\n",
      "kernel: rbf C: 10 gamma: 1 Accuracy: 0.9866666666666667\n",
      "kernel: rbf C: 100 gamma: 1 Accuracy: 0.9333333333333333\n",
      "kernel: rbf C: 1000 gamma: 1 Accuracy: 0.84\n",
      "kernel: polynomial C: 0.1 gamma: 0.001 Accuracy: 0.48\n",
      "kernel: polynomial C: 1 gamma: 0.001 Accuracy: 0.6266666666666667\n",
      "kernel: polynomial C: 10 gamma: 0.001 Accuracy: 0.6266666666666667\n",
      "kernel: polynomial C: 100 gamma: 0.001 Accuracy: 0.6266666666666667\n",
      "kernel: polynomial C: 1000 gamma: 0.001 Accuracy: 0.6533333333333333\n",
      "kernel: polynomial C: 0.1 gamma: 0.01 Accuracy: 0.48\n",
      "kernel: polynomial C: 1 gamma: 0.01 Accuracy: 0.6266666666666667\n",
      "kernel: polynomial C: 10 gamma: 0.01 Accuracy: 0.6266666666666667\n",
      "kernel: polynomial C: 100 gamma: 0.01 Accuracy: 0.6266666666666667\n",
      "kernel: polynomial C: 1000 gamma: 0.01 Accuracy: 0.6533333333333333\n",
      "kernel: polynomial C: 0.1 gamma: 0.1 Accuracy: 0.48\n",
      "kernel: polynomial C: 1 gamma: 0.1 Accuracy: 0.6266666666666667\n",
      "kernel: polynomial C: 10 gamma: 0.1 Accuracy: 0.6266666666666667\n",
      "kernel: polynomial C: 100 gamma: 0.1 Accuracy: 0.6266666666666667\n",
      "kernel: polynomial C: 1000 gamma: 0.1 Accuracy: 0.6533333333333333\n",
      "kernel: polynomial C: 0.1 gamma: 1 Accuracy: 0.48\n",
      "kernel: polynomial C: 1 gamma: 1 Accuracy: 0.6266666666666667\n",
      "kernel: polynomial C: 10 gamma: 1 Accuracy: 0.6266666666666667\n",
      "kernel: polynomial C: 100 gamma: 1 Accuracy: 0.6266666666666667\n",
      "kernel: polynomial C: 1000 gamma: 1 Accuracy: 0.6533333333333333\n",
      "kernel: sigmoid C: 0.1 gamma: 0.001 Accuracy: 0.05333333333333334\n",
      "kernel: sigmoid C: 1 gamma: 0.001 Accuracy: 0.05333333333333334\n",
      "kernel: sigmoid C: 10 gamma: 0.001 Accuracy: 0.04\n",
      "kernel: sigmoid C: 100 gamma: 0.001 Accuracy: 0.03333333333333333\n",
      "kernel: sigmoid C: 1000 gamma: 0.001 Accuracy: 0.04\n",
      "kernel: sigmoid C: 0.1 gamma: 0.01 Accuracy: 0.05333333333333334\n",
      "kernel: sigmoid C: 1 gamma: 0.01 Accuracy: 0.05333333333333334\n",
      "kernel: sigmoid C: 10 gamma: 0.01 Accuracy: 0.04\n",
      "kernel: sigmoid C: 100 gamma: 0.01 Accuracy: 0.03333333333333333\n",
      "kernel: sigmoid C: 1000 gamma: 0.01 Accuracy: 0.04\n",
      "kernel: sigmoid C: 0.1 gamma: 0.1 Accuracy: 0.05333333333333334\n",
      "kernel: sigmoid C: 1 gamma: 0.1 Accuracy: 0.05333333333333334\n",
      "kernel: sigmoid C: 10 gamma: 0.1 Accuracy: 0.04\n",
      "kernel: sigmoid C: 100 gamma: 0.1 Accuracy: 0.03333333333333333\n",
      "kernel: sigmoid C: 1000 gamma: 0.1 Accuracy: 0.04\n",
      "kernel: sigmoid C: 0.1 gamma: 1 Accuracy: 0.05333333333333334\n",
      "kernel: sigmoid C: 1 gamma: 1 Accuracy: 0.05333333333333334\n",
      "kernel: sigmoid C: 10 gamma: 1 Accuracy: 0.04\n",
      "kernel: sigmoid C: 100 gamma: 1 Accuracy: 0.03333333333333333\n",
      "kernel: sigmoid C: 1000 gamma: 1 Accuracy: 0.04\n"
     ]
    }
   ],
   "source": [
    "for kernel in ['rbf', 'polynomial', 'sigmoid']:\n",
    "    for gamma in [0.001, 0.01, 0.1, 1]:\n",
    "        for C in [0.1, 1, 10, 100, 1000]:\n",
    "            clf = SVM(type='classification',kernel = kernel, gamma= gamma, C=C)\n",
    "            clf.fit(X, y)\n",
    "            pred = clf.predict(X)\n",
    "            print('kernel:', kernel, 'C:', C, 'gamma:', gamma, 'Accuracy:', np.mean(y==pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR은 sklearn의 boston data를 사용하였다. SVM에서 type = regression으로 적용 한 뒤 모형을 적합시키고 MSE를 계산하여 비교하였다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['y'] = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['LSTAT']].values\n",
    "y = df['y'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR도 마찬가지로 kernel은 [rbf, polynomial], epsilon: [1, 0.1, 0.01, 0.001, 0.0001] 을 사용하여 각각 비교하였다. \n",
    "\n",
    "그 결과중 가장 성능이 좋은 것은 다음과 같. \n",
    "\n",
    "| Kernel | C | epsilon | MSE |\n",
    "| :------: | :-: | :-----: | :--------: |\n",
    "| rbf | 100 | 1 | 27.669 |\n",
    "| rbf | 100 | 0.1 | 27.968 |\n",
    "| rbf | 1000 | 0.01 | 27.998 |\n",
    "\n",
    "kernel은 SVC와 마찬가지로 rbf일때가 가장 성능이 좋았으며 C값은 100이나 1000이었으며, epsilon은 1, 0.1, 0.01으로 모두 다른 값이었다. 따라서 SVC와 유사하게 Kernel > C > epsilon 순으로 성능에 영향을 많이 끼치는 것으로 볼 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: rbf epsilon: 1 C: 0.1 MSE: 50.91524022866217\n",
      "Kernel: rbf epsilon: 1 C: 1 MSE: 32.904018201060474\n",
      "Kernel: rbf epsilon: 1 C: 10 MSE: 28.19950942493443\n",
      "Kernel: rbf epsilon: 1 C: 100 MSE: 27.66869241256815\n",
      "Kernel: rbf epsilon: 1 C: 1000 MSE: 30.25441538874462\n",
      "Kernel: polynomial epsilon: 1 C: 0.1 MSE: 56.41296989200502\n",
      "Kernel: polynomial epsilon: 1 C: 1 MSE: 56.24795479413233\n",
      "Kernel: polynomial epsilon: 1 C: 10 MSE: 56.247949120244414\n",
      "Kernel: polynomial epsilon: 1 C: 100 MSE: 56.247950366874264\n",
      "Kernel: polynomial epsilon: 1 C: 1000 MSE: 58.51204426380987\n",
      "Kernel: rbf epsilon: 0.1 C: 0.1 MSE: 52.384773054323496\n",
      "Kernel: rbf epsilon: 0.1 C: 1 MSE: 32.96065393164595\n",
      "Kernel: rbf epsilon: 0.1 C: 10 MSE: 28.329411140878886\n",
      "Kernel: rbf epsilon: 0.1 C: 100 MSE: 27.96831224640013\n",
      "Kernel: rbf epsilon: 0.1 C: 1000 MSE: 86.89142735661936\n",
      "Kernel: polynomial epsilon: 0.1 C: 0.1 MSE: 56.34479093618372\n",
      "Kernel: polynomial epsilon: 0.1 C: 1 MSE: 56.32599477063427\n",
      "Kernel: polynomial epsilon: 0.1 C: 10 MSE: 56.318821219165464\n",
      "Kernel: polynomial epsilon: 0.1 C: 100 MSE: 56.3188159287131\n",
      "Kernel: polynomial epsilon: 0.1 C: 1000 MSE: 57.30174733415406\n",
      "Kernel: rbf epsilon: 0.01 C: 0.1 MSE: 52.34253454627219\n",
      "Kernel: rbf epsilon: 0.01 C: 1 MSE: 32.89804914817538\n",
      "Kernel: rbf epsilon: 0.01 C: 10 MSE: 28.353244614315468\n",
      "Kernel: rbf epsilon: 0.01 C: 100 MSE: 28.033556126145292\n",
      "Kernel: rbf epsilon: 0.01 C: 1000 MSE: 27.9975753096193\n",
      "Kernel: polynomial epsilon: 0.01 C: 0.1 MSE: 56.32887679213688\n",
      "Kernel: polynomial epsilon: 0.01 C: 1 MSE: 56.312060460186515\n",
      "Kernel: polynomial epsilon: 0.01 C: 10 MSE: 56.51407368023864\n",
      "Kernel: polynomial epsilon: 0.01 C: 100 MSE: 56.90251131128733\n",
      "Kernel: polynomial epsilon: 0.01 C: 1000 MSE: 92.53784605321853\n",
      "Kernel: rbf epsilon: 0.001 C: 0.1 MSE: 52.37949166481797\n",
      "Kernel: rbf epsilon: 0.001 C: 1 MSE: 32.908956927810195\n",
      "Kernel: rbf epsilon: 0.001 C: 10 MSE: 28.339186755110173\n",
      "Kernel: rbf epsilon: 0.001 C: 100 MSE: 28.18870914276108\n",
      "Kernel: rbf epsilon: 0.001 C: 1000 MSE: 32.31841639922789\n",
      "Kernel: polynomial epsilon: 0.001 C: 0.1 MSE: 56.334748344381964\n",
      "Kernel: polynomial epsilon: 0.001 C: 1 MSE: 56.33068297087228\n",
      "Kernel: polynomial epsilon: 0.001 C: 10 MSE: 56.330697507152884\n",
      "Kernel: polynomial epsilon: 0.001 C: 100 MSE: 58.36696582360889\n",
      "Kernel: polynomial epsilon: 0.001 C: 1000 MSE: 56.86410803324638\n",
      "Kernel: rbf epsilon: 0.0001 C: 0.1 MSE: 52.37600477374753\n",
      "Kernel: rbf epsilon: 0.0001 C: 1 MSE: 32.91223315312292\n",
      "Kernel: rbf epsilon: 0.0001 C: 10 MSE: 28.364891139343193\n",
      "Kernel: rbf epsilon: 0.0001 C: 100 MSE: 29.928403947340836\n",
      "Kernel: rbf epsilon: 0.0001 C: 1000 MSE: 67.55925526060597\n",
      "Kernel: polynomial epsilon: 0.0001 C: 0.1 MSE: 56.33667685035085\n",
      "Kernel: polynomial epsilon: 0.0001 C: 1 MSE: 56.34899660696991\n",
      "Kernel: polynomial epsilon: 0.0001 C: 10 MSE: 56.462873531405876\n",
      "Kernel: polynomial epsilon: 0.0001 C: 100 MSE: 56.76926068623474\n",
      "Kernel: polynomial epsilon: 0.0001 C: 1000 MSE: 56.860306852005216\n"
     ]
    }
   ],
   "source": [
    "for kernel in ['rbf','polynomial']:\n",
    "    for C in [0.1, 1, 10, 100, 1000]:\n",
    "        reg1 = SVM(type='regression', kernel=kernel, C=C)\n",
    "        reg1.fit(X, y, epsilon= 1)\n",
    "        pred = reg1.predict(X)\n",
    "        mse = np.mean(np.square(y-pred))\n",
    "        print('Kernel:', kernel, 'epsilon: 1', 'C:', C, 'MSE:', mse)\n",
    "        \n",
    "for kernel in ['rbf','polynomial']:\n",
    "    for C in [0.1, 1, 10, 100, 1000]:\n",
    "        reg2 = SVM(type='regression', kernel=kernel, C=C)\n",
    "        reg2.fit(X, y, epsilon= 0.1)\n",
    "        pred = reg2.predict(X)\n",
    "        mse = np.mean(np.square(y-pred))\n",
    "        print('Kernel:', kernel, 'epsilon: 0.1', 'C:', C, 'MSE:', mse)\n",
    "\n",
    "for kernel in ['rbf','polynomial']:\n",
    "    for C in [0.1, 1, 10, 100, 1000]:\n",
    "        reg3 = SVM(type='regression', kernel=kernel, C=C)\n",
    "        reg3.fit(X, y, epsilon= 0.01)\n",
    "        pred = reg3.predict(X)\n",
    "        mse = np.mean(np.square(y-pred))\n",
    "        print('Kernel:', kernel, 'epsilon: 0.01', 'C:', C, 'MSE:', mse)\n",
    "\n",
    "for kernel in ['rbf','polynomial']:\n",
    "    for C in [0.1, 1, 10, 100, 1000]:\n",
    "        reg4 = SVM(type='regression', kernel=kernel, C=C)\n",
    "        reg4.fit(X, y, epsilon= 0.001)\n",
    "        pred = reg4.predict(X)\n",
    "        mse = np.mean(np.square(y-pred))\n",
    "        print('Kernel:', kernel, 'epsilon: 0.001', 'C:', C, 'MSE:', mse)\n",
    "\n",
    "for kernel in ['rbf','polynomial']:\n",
    "    for C in [0.1, 1, 10, 100, 1000]:\n",
    "        reg5 = SVM(type='regression', kernel=kernel, C=C)\n",
    "        reg5.fit(X, y, epsilon= 0.0001)\n",
    "        pred = reg5.predict(X)\n",
    "        mse = np.mean(np.square(y-pred))\n",
    "        print('Kernel:', kernel, 'epsilon: 0.0001', 'C:', C, 'MSE:', mse)        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "7e9ae56faa1c3d35c6ee6676d8e1566dbba0c62cdb3625d18794d30c5c8751de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
